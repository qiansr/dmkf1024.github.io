<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>程序员技术栈</title>
    <link>http://dmkf.xyz/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>一名默默无闻的程序猿的日常博客分享</description>
    <pubDate>Mon, 24 Apr 2017 05:19:58 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>Eclipse新版本无法安装fatjar插件问题解决</title>
      <link>http://dmkf.xyz/2017/04/24/11689085/</link>
      <guid>http://dmkf.xyz/2017/04/24/11689085/</guid>
      <pubDate>Mon, 24 Apr 2017 04:50:02 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;前情提要&quot;&gt;&lt;a href=&quot;#前情提要&quot; class=&quot;headerlink&quot; title=&quot;前情提要&quot;&gt;&lt;/a&gt;前情提要&lt;/h1&gt;&lt;p&gt;今天将Java程序导出为Jar包的时候遇到一个问题：程序中写了一个DBUtil类，在Eclipse中直接跑的时候没有任何问题
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h1><p>今天将Java程序导出为Jar包的时候遇到一个问题：程序中写了一个DBUtil类，在Eclipse中直接跑的时候没有任何问题，然而导出为Jar的时候发现运行报出找不到依赖包的问题，查询了网上，发现Eclipse的fat-jar插件可以将依赖包自动导入进去，于是尝试下载Eclipse插件。这时候遇到了问题，笔者的Eclipse版本无法安装fat-jar插件。</p>
<h1 id="下载安装Eclipse2-0版本的插件支持"><a href="#下载安装Eclipse2-0版本的插件支持" class="headerlink" title="下载安装Eclipse2.0版本的插件支持"></a>下载安装Eclipse2.0版本的插件支持</h1><p>插件之所以无法安装是因为插件的版本和Eclipse的版本不兼容，这时候需要下载一个另一个插件，来解决新老版本的插件无法安装完问题。<br>打开Eclipse，菜单栏找到Help–Install New Software，在弹出框中单击Add…按钮：<br><img src="http://upload-images.jianshu.io/upload_images/291600-bc47cec10fbbeacc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/450" alt=""></p>
<p>单击按钮后出现新的弹出窗口，分别输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Name: The Eclipse Project Updates </div><div class="line">Location: http://download.eclipse.org/eclipse/updates/4.4</div></pre></td></tr></table></figure>
<p>如下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-93d1a8cdeef82034.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/450" alt=""></p>
<p>然后单击OK按钮，接着一路Next完成插件的安装后，重启Eclipse即可。</p>
<h1 id="下载安装fatjar插件"><a href="#下载安装fatjar插件" class="headerlink" title="下载安装fatjar插件"></a>下载安装fatjar插件</h1><p>以上述讨论，输入如下内容即可完成fatjar插件的安装。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Name: fatjar</div><div class="line">Location: http://kurucz-grafika.de/fatjar</div></pre></td></tr></table></figure>
<h1 id="fatjar的使用"><a href="#fatjar的使用" class="headerlink" title="fatjar的使用"></a>fatjar的使用</h1><p>既然已经提到了fatjar的安装，就在提提fatjar的使用吧！<br>单击右键选择Export，在出现的弹窗中选择Other–Fat Jar Exporter。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-a0ad5b0918628255.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/450" alt=""></p>
<p>在Select one java Project窗口选择要导出的Java工程，单击Next按钮：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-29d8337f455bbfad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/450" alt=""></p>
<p>在Main-Class一项单击Browse…按钮，选择程序入口的类（包含main方法的类），然后勾选One-JAR选框</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-b6476d9d0e5878f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/450" alt=""></p>
<p>然后选择要一起打包的依赖包，单击Finish按钮即可完成导出Jar的过程。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-d0dce155aace53c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/450" alt=""></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://blog.csdn.net/lvxiangan/article/details/54019914" target="_blank" rel="external">解决Eclipse最新版本无法安装fatjar插件的问题</a><br><a href="http://www.cnblogs.com/mstk/p/3884195.html" target="_blank" rel="external">Eclipse下FatJar插件的安装与使用</a></p>
]]></content:encoded>
      
      <comments>http://dmkf.xyz/2017/04/24/11689085/#disqus_thread</comments>
    </item>
    
    <item>
      <title>【门外汉深度学习-7】深度信念网络</title>
      <link>http://dmkf.xyz/2017/04/02/10881655/</link>
      <guid>http://dmkf.xyz/2017/04/02/10881655/</guid>
      <pubDate>Sun, 02 Apr 2017 04:08:12 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本文源自YouTube网站《&lt;a href=&quot;https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;amp
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文源自YouTube网站《<a href="https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;index=1&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu" target="_blank" rel="external">Deep Learning SIMPLIFIED</a>》系列视频，文章内容为视频的字幕，供深度学习初学者参考学习。</p>
<h1 id="主体内容"><a href="#主体内容" class="headerlink" title="主体内容"></a>主体内容</h1><p><img src="http://upload-images.jianshu.io/upload_images/291600-7baff08f5ba6ae70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Okay, so an <strong><em>RBM</em></strong>（Restricted Boltzmann Machine，受限波兹曼机） can extract features and reconstruct inputs…but how exactly does that help with the <strong>vanishing gradient</strong>（消失梯度）? By combining <strong><em>RBMs</em></strong> together and introducing a clever training method, we obtain a powerful new model that finally solves our problem. Let’s now take a look at a <strong>Deep Belief Network</strong>（深度信念网络）.</p>
<p>Just like the <strong><em>RBM</em></strong>, <strong>Deep Belief Nets</strong> were also conceived by Geoff Hinton as an alternative to <strong>backpropagation</strong>（反向传播）. Because of his accomplishments, he was hired for image recognition work at Google, where a large-scale <strong><em>DBN</em></strong>（Deep Belief Nets）project is currently believed to be in development.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-2f75f9a08f324bb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>In terms of network structure, a <strong><em>DBN</em></strong> is identical to an <strong><em>MLP</em></strong>（Multi-layer Perceptron，多层感知器）. But when it comes to training, they are entirely different. In fact, the difference in training methods is the key factor that enables <strong><em>DBNs</em></strong> to outperform their shallow counterparts.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-b7d5db9a8654d62c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>A <strong>deep belief network</strong> can be viewed as a stack of <strong><em>RBMs</em></strong>, where the hidden layer of one <strong><em>RBM</em></strong> is the visible layer of the one “above” it. A <strong><em>DBN</em></strong> is trained as follows:<br><strong>a)</strong> The first <strong><em>RBM</em></strong> is trained to re-construct its input as accurately as possible.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-1c96aa08cd150c40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p><strong>b)</strong> The hidden layer of the first <strong><em>RBM</em></strong> is treated as the visible layer of the second and the second <strong><em>RBM</em></strong> is trained using the outputs from using the outputs from the first <strong><em>RBM</em></strong>.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-09b73f3625364971.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p><strong>c)</strong> This process is repeated until every layer in the network is trained.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-9b863af94b1c80c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>An important note about a <strong><em>DBN</em></strong> is that each <strong><em>RBM</em></strong> layer learns the entire input. In other kinds of models - like <strong>convolutional nets</strong>（卷积网络） - early layers detect simple patterns and later layers recombine them.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-7e83979257c2fc36.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Like in our facial recognition example, the early layers would detect edges in the image, and later layers would use these results to form facial features. A <strong><em>DBN</em></strong>, on the other hand, works globally by fine tuning the entire input in succession as the model slowly improves - kind of like a camera lens slowly focusing a picture. The reason that a <strong><em>DBN</em></strong> works so well is highly technical, but suffice it to say that a stack of <strong><em>RBMs</em></strong> will outperform a single unit - just like a <strong>Multilayer perceptron</strong>（多层感知器） was able to outperform a single perceptron working alone.</p>
<p>After this initial training, the <strong><em>RBMs</em></strong> have created a model that can detect inherent patterns in the data. But we still don’t know exactly what the patterns are called. To finish training, we need to introduce labels to the patterns and fine-tune the net with supervised learning. To do this, you need a very small set of labelled samples so that the features and patterns can be associated with a name. The weights and biases are altered slightly, resulting in a small change in the net’s perception of the patterns, and often a small increase in the total accuracy. Fortunately, the set of labelled data can be small relative to the original data set, which as we’ve discussed, is extremely helpful in real-world applications.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-5d790f31f280a822.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Have you ever trained a <strong>Deep Belief Network</strong> before? If so, please comment and share your experiences.</p>
<p>So let’s recap the benefits of a <strong>Deep Belief Network</strong>. We saw that a <strong><em>DBN</em></strong> only needs a small labelled data set, which is important for real-world applications. The training process can also be completed in a reasonable amount of time through the use of GPUs. And best of all, the resulting net will be very accurate compared to a shallow net, so we can finally see that a <strong><em>DBN</em></strong> is a solution to the <strong>vanishing gradient</strong> problem!</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-58db0cd067315198.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Now that we’ve covered the <strong>Deep Belief Net</strong>, we can begin to discuss a few other deep learning models and see what kinds of problems they can solve. Let’s jump over to the next video, and take a look at how a <strong>convolutional net</strong> could be trained to recognize different objects in a image.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-f7ab81d3c318996b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><p><a href="https://www.youtube.com/watch?v=E2Mt_7qked0&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu&amp;index=7" target="_blank" rel="external">视频链接</a><br><a href="http://www.jianshu.com/p/4f7e4f27dad9" target="_blank" rel="external">本系列文章目录</a></p>
]]></content:encoded>
      
      <comments>http://dmkf.xyz/2017/04/02/10881655/#disqus_thread</comments>
    </item>
    
    <item>
      <title>【门外汉深度学习-6】受限波兹曼机</title>
      <link>http://dmkf.xyz/2017/04/02/10792711/</link>
      <guid>http://dmkf.xyz/2017/04/02/10792711/</guid>
      <pubDate>Sun, 02 Apr 2017 04:06:44 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本文源自YouTube网站《&lt;a href=&quot;https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;amp
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文源自YouTube网站《<a href="https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;index=1&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu" target="_blank" rel="external">Deep Learning SIMPLIFIED</a>》系列视频，文章内容为视频的字幕，供深度学习初学者参考学习。</p>
<h1 id="主体内容"><a href="#主体内容" class="headerlink" title="主体内容"></a>主体内容</h1><p><img src="http://upload-images.jianshu.io/upload_images/291600-a6b12869c3211a56.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>So what was it that allowed researchers to overcome the <strong>vanishing gradient</strong> problem? The answer to this question has two parts, the first of which involves a <strong>Restricted Boltzmann Machine</strong>（受限波兹曼机）. This is a method that can automatically find patterns in our data by reconstructing the input. Sounds complicated, but bear with me. We’ll take a closer look.</p>
<p>Geoff Hinton at the University of Toronto was one of the first researchers to devise a breakthrough idea for training deep nets. His approach led to the creation of the <strong>Restricted Boltzmann Machine</strong>, also known as the <strong><em>RBM</em></strong>. Because of his pioneering work he’s often referred to as one of the father’s of deep learning.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-bc6843948cbcebcf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>An <strong><em>RBM</em></strong> is a shallow, two-layer net; the first layer is known as the visible layer and the second is called the hidden layer. Each node in the visible layer is connected to every node in the hidden layer. An <strong><em>RBM</em></strong> is considered “<strong>restricted</strong>“ because no two nodes in the same layer share a connection. An <strong><em>RBM</em></strong> is the mathematical equivalent of a two-way translator - in the forward pass, an <strong><em>RBM</em></strong> takes the inputs and translates them into a set of numbers that encode the inputs. In the backward pass, it takes this set of numbers and translates them back to form the re-constructed inputs. A well-trained net will be able to perform the backwards translation with a high degree of accuracy. In both steps, the <strong>weights</strong> and <strong>biases</strong> have a very important role. They allow the <strong><em>RBM</em></strong> to decipher the interrelationships among the input features, and they also help the <strong><em>RBM</em></strong> decide which input features are the most important when detecting patterns.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-0884b519dd82b7c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Through several forward and backward passes, an <strong><em>RBM</em></strong> is trained to reconstruct the input data. Three steps are repeated over and over through the training process:<br><strong>a)</strong> With a forward pass, every input is combined with an individual weight and one overall bias, and the result is passed to the hidden layer which may or may not activate. Here’s how it flows for the whole net.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-50ec6218846a61bb.gif?imageMogr2/auto-orient/strip" alt=""></p>
<p><strong>b)</strong> Next, in a backward pass, each activation is combined with an individual weight and an overall bias, and the result is passed to the visible layer for reconstruction. Here’s how it flows back.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-eab7507ac1781c04.gif?imageMogr2/auto-orient/strip" alt=""></p>
<p><strong>c)</strong> At the visible layer, the reconstruction is compared against the original input to determine the quality of the result. <strong><em>RBMs</em></strong> use a measure called <strong>KL Divergence</strong> for step c); steps a) thru c) are repeated with varying weights and biases until the input and the re-construction are as close as possible. Have you ever had to train an <strong><em>RBM</em></strong> in one of your own machine learning projects? Please comment and tell me about your experiences.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-e0923f243a68d4e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>An interesting aspect of an <strong><em>RBM</em></strong> is that the data does not need to be labelled. This turns out to be very important for real-world data sets like photos, videos, voices, and sensor data - all of which tend to be unlabelled. Rather than having people manually label the data and introduce errors, an <strong><em>RBM</em></strong> automatically sorts through the data, and by properly adjusting the weights and biases, an <strong><em>RBM</em></strong> is able to extract the important features and reconstruct the input. An important note is that an <strong><em>RBM</em></strong> is actually make decisions about which input features are important and how they should be combined to form patterns. In other words, an <strong><em>RBM</em></strong> is part of a family of feature extractor neural nets, which are all designed to recognize inherent patterns in data. These nets are also called <strong>autoencoders</strong>, because in a way, they have to encode their own structure.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-09ca1c4fe0b95c91.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>So we say that an <strong><em>RBM</em></strong> can extract features, but how does that help with the vanishing gradient? We’ll get to the second part of our answer in the next video when we take a look at the <strong>Deep Belief Net</strong>.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-f7ab81d3c318996b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><p><a href="https://www.youtube.com/watch?v=puux7KZQfsE&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu&amp;index=6" target="_blank" rel="external">视频链接</a><br><a href="http://www.jianshu.com/p/4f7e4f27dad9" target="_blank" rel="external">本系列文章目录</a></p>
]]></content:encoded>
      
      <comments>http://dmkf.xyz/2017/04/02/10792711/#disqus_thread</comments>
    </item>
    
    <item>
      <title>【门外汉深度学习-5】一个老问题</title>
      <link>http://dmkf.xyz/2017/04/02/10787217/</link>
      <guid>http://dmkf.xyz/2017/04/02/10787217/</guid>
      <pubDate>Sun, 02 Apr 2017 04:05:38 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本文源自YouTube网站《&lt;a href=&quot;https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;amp
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文源自YouTube网站《<a href="https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;index=1&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu" target="_blank" rel="external">Deep Learning SIMPLIFIED</a>》系列视频，文章内容为视频的字幕，供深度学习初学者参考学习。</p>
<h1 id="主体内容"><a href="#主体内容" class="headerlink" title="主体内容"></a>主体内容</h1><p><img src="http://upload-images.jianshu.io/upload_images/291600-18f5baa567c324bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>So now you’re probably thinking - wow, deep nets are really great! But why did it take so long for them to become popular? Well as it turns out, when you try to train them with a method called <strong>backpropagation</strong>（反向传播）, you run into a fundamental problem called the <strong>vanishing gradient</strong>（消失梯度）, or sometimes the <strong>exploding gradient</strong>（爆发梯度）. When that happens, training takes too long and the accuracy really suffers. Let’s take a closer look.</p>
<p>When you’re training a neural net, you’re constantly calculating a cost value. The cost is typically the difference between the net’s predicted output and the actual output from a set of labelled training data. The cost is then lowered by making slight adjustments to the weights and biases over and over throughout the training process, until the lowest possible value is obtained. Here is that forward prop again; and here are the example weights and biases.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-9f1aa39cff6f8bd6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>The training process utilizes something called a <strong>gradient</strong>（梯度）, which measures the rate at which the cost will change with respect to a change in a weight or a bias.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-32682b72029d39d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p><strong>Deep architectures</strong> are your best and sometimes your only choice for complex machine learning problems such as facial recognition. But up until 2006, there was no way to accurately train deep nets due to a fundamental problem with the training process: the <strong>vanishing gradient</strong>.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-51768f97898e6cec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Let’s think of a gradient like a slope, and the training process like a rock rolling down that slope. A rock will roll quickly down a steep slope but will barely move at all on a flat surface. The same is true with the gradient of a deep net. When the gradient is large, the net will train quickly. When the gradient is small, the net will train slowly.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-10dc44a68b2f3eeb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Here’s that deep net again.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-f662f490a0d10129.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>And here is how the gradient could potentially vanish or decay back through the net. As you can see, the gradients are much small in the earlier layers. As a result, the early layers of the network are the slowest to train. But this is a fundamental problem! The early layers are responsible for detecting the simple patterns and the building blocks -when it came to facial recognition, the early layers detected the edges which were combined to form facial features later in the network. </p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-4eb36da118574ae0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>And if the early layers get it wrong, the result built up by the net will be wrong as well. It could mean that instead of a face like this, your net looks for this.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-6779922e5ad7b679.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>The process used for training a neural net is called <strong>back-propagation</strong> or <strong>back-prop</strong>（反向传播）. We saw before that <strong>forward prop</strong> starts with the inputs and works forward; <strong>back-prop</strong> does the reverse, calculating the gradient from right to left.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-6f8e9ff65bf631f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>For example, here are 5 gradients, 4 weights and 1 bias. </p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-23bc396e09d8139f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>It starts with the left and works back through the layers, like so. Each time it calculates a gradient, it uses all the previous gradients up to that point.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-44f697c99f9eabd0.gif?imageMogr2/auto-orient/strip" alt=""></p>
<p>So, let’s start with that node. That edge uses the gradient at that node. And the next. So far things are simple.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-a2d602cf560907fd.gif?imageMogr2/auto-orient/strip" alt=""></p>
<p>As you keep going back, things get a bit more complex - that one for example uses a lot of gradients, even though this is a relatively simple net. If your net gets larger and deeper, like this one, it gets even worse.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-de5c3d9af77a018b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>But why is that? Well, a gradient at any point is the product of the previous gradients up to that point. And the product of two numbers between 0 and 1 gives you a smaller number.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-a7a4f7af3c2a6410.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p> Say this rectangle is a one. </p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-265816c3fc9d282e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Also, say there are two gradients - a fourth - like that</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-e99e26fe47452fe4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<ul>
<li>and a third.</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-b09b63e8b060056c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>If you multiply them, you get a fourth of a third which is a twelfth.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-e27dc07fcdbeaa49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>A fourth of a twelfth is a forty-eighth.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-15d7e3479beea24a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>You can see that numbers keep getting smaller the more you multiply. Have you ever had this issue while training a neural network with <strong>backpropagation</strong>? If so, please comment and let me know your thoughts.</p>
<p>As a result of all this, <strong>backprop</strong> ends up taking a lot of time to train the net, and the accuracy is often very low.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-64c3c64229a301a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Up until 2006, deep nets were still underperforming shallow nets and other machine learning algorithms. But everything changed after three breakthrough papers published by Hinton, Lecun, and Bengio in 2006 and 2007.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-4391e6c11ad44c66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>In the next video, we’ll begin taking a closer look at these breakthroughs, starting with the <strong>Restricted Boltzmann Machine</strong>（受限波兹曼机）.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-f7ab81d3c318996b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><p><a href="https://www.youtube.com/watch?v=SKMpmAOUa2Q&amp;index=5&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu" target="_blank" rel="external">视频链接</a><br><a href="http://www.jianshu.com/p/4f7e4f27dad9" target="_blank" rel="external">本系列文章目录</a></p>
<h1 id="拓展阅读"><a href="#拓展阅读" class="headerlink" title="拓展阅读"></a>拓展阅读</h1><ul>
<li><a href="http://blog.csdn.net/qq_29133371/article/details/51867856" target="_blank" rel="external">关于梯度消失，梯度爆炸的问题</a></li>
<li><a href="https://my.oschina.net/findbill/blog/529001" target="_blank" rel="external">反向传播（Backpropagation）算法的数学原理</a></li>
<li><a href="https://www.zhihu.com/question/27239198" target="_blank" rel="external">如何直观的解释back propagation算法？</a></li>
<li><a href="http://blog.csdn.net/dingchenxixi/article/details/51371641" target="_blank" rel="external">Coursera机器学习-第四周-Neural Network ForwardPropagation</a></li>
</ul>
]]></content:encoded>
      
      <comments>http://dmkf.xyz/2017/04/02/10787217/#disqus_thread</comments>
    </item>
    
    <item>
      <title>【门外汉深度学习-4】深度网络的选择</title>
      <link>http://dmkf.xyz/2017/04/02/10765634/</link>
      <guid>http://dmkf.xyz/2017/04/02/10765634/</guid>
      <pubDate>Sun, 02 Apr 2017 04:03:54 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本文源自YouTube网站《&lt;a href=&quot;https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;amp
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文源自YouTube网站《<a href="https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;index=1&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu" target="_blank" rel="external">Deep Learning SIMPLIFIED</a>》系列视频，文章内容为视频的字幕，供深度学习初学者参考学习。</p>
<h1 id="主体内容"><a href="#主体内容" class="headerlink" title="主体内容"></a>主体内容</h1><p><img src="http://upload-images.jianshu.io/upload_images/291600-619b127bca41e211.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Now if you’re like me, starting a Deep Learning project sounds really exciting. But when it comes to picking the right kind of net to use, well, things can get a little confusing. You first need to decide if you’re trying to build a classifier or if you’re trying to find patterns in your data. Beyond that, I’ll try to help by giving you some general guidelines.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-76939354a6feb347.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Before we get started, I want to give you a bit of a heads up. I’m going to be using some terminology that may sound a little scary right now, but don’t worry. I’ll cover all these terms in detail in the upcoming videos. If you’re interested in unsupervised learning - that is, you want to extract patterns from a set of unlabelled data - then your best bet is to use either a <strong>Restricted Boltzmann Machine</strong>（受限波兹曼机）, or an <strong>autoencoder</strong>（自编码器）. What type of projects would you need to use a Deep Net for? Please comment and let me know your thoughts.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-7884bb2e63a021b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>If you have labeled data for supervised learning and you want to build a classifier, you have several different options depending on your application.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-823c5c38d048857c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>For text processing tasks like sentiment analysis, parsing, and named entity recognition - use a <strong>Recurrent Net</strong> or a <strong>Recursive Neural Tensor Network</strong>（递归神经张量网络）,  which we’ll refer to as an <strong><em>RNTN</em></strong>. For any language model that operates on the character level, use a <strong>Recurrent Net</strong>.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-b1a4ef267c3ec0cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>For image recognition, use a <strong>Deep Belief Network</strong>（深度信念网络） or a <strong>Convolutional Net</strong>（卷积网络）.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-497cefdaca33e995.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>For object recognition, use a <strong>Convolutional Net</strong> or an <strong><em>RNTN</em></strong>.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-fe7fb3ba4da4746e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Finally, for speech recognition, use a <strong>Recurrent Net</strong>.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-7864a9362e15672b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Ingeneral, <strong>Deep Belief Networks</strong> and <strong>Multilayer Perceptrons</strong>（多层感知机） with rectified linear units - also known as <strong><em>RELU</em></strong> - are both good choices for classification. For time series analysis, it’s best to use a <strong>Recurrent Net</strong>. </p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-edc84d593ae3c095.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Deep Nets are the current state of the art in pattern recognition, but it’s worth noting that neural nets have been around for decades.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-da7f39818a2bacf7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p> So you might be wondering: why did it take almost 50 years for Deep Nets to come on to the scene? Well, as it turns out, Deep Nets are very hard to train, which we will see in the next video.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-f7ab81d3c318996b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><p><a href="https://www.youtube.com/watch?v=JjZDoojyzXQ&amp;index=4&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu" target="_blank" rel="external">视频链接</a><br><a href="http://www.jianshu.com/p/4f7e4f27dad9" target="_blank" rel="external">本系列文章目录</a></p>
<h1 id="拓展阅读"><a href="#拓展阅读" class="headerlink" title="拓展阅读"></a>拓展阅读</h1><ul>
<li><a href="http://www.cnblogs.com/kemaswill/p/3266026.html" target="_blank" rel="external">Deep Belief Network简介</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_69e2aa270102ygvh.html" target="_blank" rel="external">CNN&amp;RELU&amp;POOL 感觉没有比这篇文章讲的更详细了</a></li>
</ul>
]]></content:encoded>
      
      <comments>http://dmkf.xyz/2017/04/02/10765634/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
