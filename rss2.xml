<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>程序员技术栈</title>
    <link>http://inerdstack.com/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>一名默默无闻的程序猿的日常博客分享</description>
    <pubDate>Sun, 02 Apr 2017 04:09:10 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>【门外汉深度学习-7】深度信念网络</title>
      <link>http://inerdstack.com/2017/04/02/10881655/</link>
      <guid>http://inerdstack.com/2017/04/02/10881655/</guid>
      <pubDate>Sun, 02 Apr 2017 04:08:12 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本文源自YouTube网站《&lt;a href=&quot;https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;amp
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文源自YouTube网站《<a href="https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;index=1&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu" target="_blank" rel="external">Deep Learning SIMPLIFIED</a>》系列视频，文章内容为视频的字幕，供深度学习初学者参考学习。</p>
<h1 id="主体内容"><a href="#主体内容" class="headerlink" title="主体内容"></a>主体内容</h1><p><img src="http://upload-images.jianshu.io/upload_images/291600-7baff08f5ba6ae70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Okay, so an <strong><em>RBM</em></strong>（Restricted Boltzmann Machine，受限波兹曼机） can extract features and reconstruct inputs…but how exactly does that help with the <strong>vanishing gradient</strong>（消失梯度）? By combining <strong><em>RBMs</em></strong> together and introducing a clever training method, we obtain a powerful new model that finally solves our problem. Let’s now take a look at a <strong>Deep Belief Network</strong>（深度信念网络）.</p>
<p>Just like the <strong><em>RBM</em></strong>, <strong>Deep Belief Nets</strong> were also conceived by Geoff Hinton as an alternative to <strong>backpropagation</strong>（反向传播）. Because of his accomplishments, he was hired for image recognition work at Google, where a large-scale <strong><em>DBN</em></strong>（Deep Belief Nets）project is currently believed to be in development.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-2f75f9a08f324bb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>In terms of network structure, a <strong><em>DBN</em></strong> is identical to an <strong><em>MLP</em></strong>（Multi-layer Perceptron，多层感知器）. But when it comes to training, they are entirely different. In fact, the difference in training methods is the key factor that enables <strong><em>DBNs</em></strong> to outperform their shallow counterparts.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-b7d5db9a8654d62c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>A <strong>deep belief network</strong> can be viewed as a stack of <strong><em>RBMs</em></strong>, where the hidden layer of one <strong><em>RBM</em></strong> is the visible layer of the one “above” it. A <strong><em>DBN</em></strong> is trained as follows:<br><strong>a)</strong> The first <strong><em>RBM</em></strong> is trained to re-construct its input as accurately as possible.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-1c96aa08cd150c40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p><strong>b)</strong> The hidden layer of the first <strong><em>RBM</em></strong> is treated as the visible layer of the second and the second <strong><em>RBM</em></strong> is trained using the outputs from using the outputs from the first <strong><em>RBM</em></strong>.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-09b73f3625364971.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p><strong>c)</strong> This process is repeated until every layer in the network is trained.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-9b863af94b1c80c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>An important note about a <strong><em>DBN</em></strong> is that each <strong><em>RBM</em></strong> layer learns the entire input. In other kinds of models - like <strong>convolutional nets</strong>（卷积网络） - early layers detect simple patterns and later layers recombine them.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-7e83979257c2fc36.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Like in our facial recognition example, the early layers would detect edges in the image, and later layers would use these results to form facial features. A <strong><em>DBN</em></strong>, on the other hand, works globally by fine tuning the entire input in succession as the model slowly improves - kind of like a camera lens slowly focusing a picture. The reason that a <strong><em>DBN</em></strong> works so well is highly technical, but suffice it to say that a stack of <strong><em>RBMs</em></strong> will outperform a single unit - just like a <strong>Multilayer perceptron</strong>（多层感知器） was able to outperform a single perceptron working alone.</p>
<p>After this initial training, the <strong><em>RBMs</em></strong> have created a model that can detect inherent patterns in the data. But we still don’t know exactly what the patterns are called. To finish training, we need to introduce labels to the patterns and fine-tune the net with supervised learning. To do this, you need a very small set of labelled samples so that the features and patterns can be associated with a name. The weights and biases are altered slightly, resulting in a small change in the net’s perception of the patterns, and often a small increase in the total accuracy. Fortunately, the set of labelled data can be small relative to the original data set, which as we’ve discussed, is extremely helpful in real-world applications.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-5d790f31f280a822.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Have you ever trained a <strong>Deep Belief Network</strong> before? If so, please comment and share your experiences.</p>
<p>So let’s recap the benefits of a <strong>Deep Belief Network</strong>. We saw that a <strong><em>DBN</em></strong> only needs a small labelled data set, which is important for real-world applications. The training process can also be completed in a reasonable amount of time through the use of GPUs. And best of all, the resulting net will be very accurate compared to a shallow net, so we can finally see that a <strong><em>DBN</em></strong> is a solution to the <strong>vanishing gradient</strong> problem!</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-58db0cd067315198.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Now that we’ve covered the <strong>Deep Belief Net</strong>, we can begin to discuss a few other deep learning models and see what kinds of problems they can solve. Let’s jump over to the next video, and take a look at how a <strong>convolutional net</strong> could be trained to recognize different objects in a image.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-f7ab81d3c318996b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><p><a href="https://www.youtube.com/watch?v=E2Mt_7qked0&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu&amp;index=7" target="_blank" rel="external">视频链接</a><br><a href="http://www.jianshu.com/p/4f7e4f27dad9" target="_blank" rel="external">本系列文章目录</a></p>
]]></content:encoded>
      
      <comments>http://inerdstack.com/2017/04/02/10881655/#disqus_thread</comments>
    </item>
    
    <item>
      <title>【门外汉深度学习-6】受限波兹曼机</title>
      <link>http://inerdstack.com/2017/04/02/10792711/</link>
      <guid>http://inerdstack.com/2017/04/02/10792711/</guid>
      <pubDate>Sun, 02 Apr 2017 04:06:44 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本文源自YouTube网站《&lt;a href=&quot;https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;amp
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文源自YouTube网站《<a href="https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;index=1&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu" target="_blank" rel="external">Deep Learning SIMPLIFIED</a>》系列视频，文章内容为视频的字幕，供深度学习初学者参考学习。</p>
<h1 id="主体内容"><a href="#主体内容" class="headerlink" title="主体内容"></a>主体内容</h1><p><img src="http://upload-images.jianshu.io/upload_images/291600-a6b12869c3211a56.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>So what was it that allowed researchers to overcome the <strong>vanishing gradient</strong> problem? The answer to this question has two parts, the first of which involves a <strong>Restricted Boltzmann Machine</strong>（受限波兹曼机）. This is a method that can automatically find patterns in our data by reconstructing the input. Sounds complicated, but bear with me. We’ll take a closer look.</p>
<p>Geoff Hinton at the University of Toronto was one of the first researchers to devise a breakthrough idea for training deep nets. His approach led to the creation of the <strong>Restricted Boltzmann Machine</strong>, also known as the <strong><em>RBM</em></strong>. Because of his pioneering work he’s often referred to as one of the father’s of deep learning.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-bc6843948cbcebcf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>An <strong><em>RBM</em></strong> is a shallow, two-layer net; the first layer is known as the visible layer and the second is called the hidden layer. Each node in the visible layer is connected to every node in the hidden layer. An <strong><em>RBM</em></strong> is considered “<strong>restricted</strong>“ because no two nodes in the same layer share a connection. An <strong><em>RBM</em></strong> is the mathematical equivalent of a two-way translator - in the forward pass, an <strong><em>RBM</em></strong> takes the inputs and translates them into a set of numbers that encode the inputs. In the backward pass, it takes this set of numbers and translates them back to form the re-constructed inputs. A well-trained net will be able to perform the backwards translation with a high degree of accuracy. In both steps, the <strong>weights</strong> and <strong>biases</strong> have a very important role. They allow the <strong><em>RBM</em></strong> to decipher the interrelationships among the input features, and they also help the <strong><em>RBM</em></strong> decide which input features are the most important when detecting patterns.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-0884b519dd82b7c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Through several forward and backward passes, an <strong><em>RBM</em></strong> is trained to reconstruct the input data. Three steps are repeated over and over through the training process:<br><strong>a)</strong> With a forward pass, every input is combined with an individual weight and one overall bias, and the result is passed to the hidden layer which may or may not activate. Here’s how it flows for the whole net.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-50ec6218846a61bb.gif?imageMogr2/auto-orient/strip" alt=""></p>
<p><strong>b)</strong> Next, in a backward pass, each activation is combined with an individual weight and an overall bias, and the result is passed to the visible layer for reconstruction. Here’s how it flows back.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-eab7507ac1781c04.gif?imageMogr2/auto-orient/strip" alt=""></p>
<p><strong>c)</strong> At the visible layer, the reconstruction is compared against the original input to determine the quality of the result. <strong><em>RBMs</em></strong> use a measure called <strong>KL Divergence</strong> for step c); steps a) thru c) are repeated with varying weights and biases until the input and the re-construction are as close as possible. Have you ever had to train an <strong><em>RBM</em></strong> in one of your own machine learning projects? Please comment and tell me about your experiences.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-e0923f243a68d4e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>An interesting aspect of an <strong><em>RBM</em></strong> is that the data does not need to be labelled. This turns out to be very important for real-world data sets like photos, videos, voices, and sensor data - all of which tend to be unlabelled. Rather than having people manually label the data and introduce errors, an <strong><em>RBM</em></strong> automatically sorts through the data, and by properly adjusting the weights and biases, an <strong><em>RBM</em></strong> is able to extract the important features and reconstruct the input. An important note is that an <strong><em>RBM</em></strong> is actually make decisions about which input features are important and how they should be combined to form patterns. In other words, an <strong><em>RBM</em></strong> is part of a family of feature extractor neural nets, which are all designed to recognize inherent patterns in data. These nets are also called <strong>autoencoders</strong>, because in a way, they have to encode their own structure.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-09ca1c4fe0b95c91.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>So we say that an <strong><em>RBM</em></strong> can extract features, but how does that help with the vanishing gradient? We’ll get to the second part of our answer in the next video when we take a look at the <strong>Deep Belief Net</strong>.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-f7ab81d3c318996b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><p><a href="https://www.youtube.com/watch?v=puux7KZQfsE&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu&amp;index=6" target="_blank" rel="external">视频链接</a><br><a href="http://www.jianshu.com/p/4f7e4f27dad9" target="_blank" rel="external">本系列文章目录</a></p>
]]></content:encoded>
      
      <comments>http://inerdstack.com/2017/04/02/10792711/#disqus_thread</comments>
    </item>
    
    <item>
      <title>【门外汉深度学习-5】一个老问题</title>
      <link>http://inerdstack.com/2017/04/02/10787217/</link>
      <guid>http://inerdstack.com/2017/04/02/10787217/</guid>
      <pubDate>Sun, 02 Apr 2017 04:05:38 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本文源自YouTube网站《&lt;a href=&quot;https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;amp
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文源自YouTube网站《<a href="https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;index=1&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu" target="_blank" rel="external">Deep Learning SIMPLIFIED</a>》系列视频，文章内容为视频的字幕，供深度学习初学者参考学习。</p>
<h1 id="主体内容"><a href="#主体内容" class="headerlink" title="主体内容"></a>主体内容</h1><p><img src="http://upload-images.jianshu.io/upload_images/291600-18f5baa567c324bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>So now you’re probably thinking - wow, deep nets are really great! But why did it take so long for them to become popular? Well as it turns out, when you try to train them with a method called <strong>backpropagation</strong>（反向传播）, you run into a fundamental problem called the <strong>vanishing gradient</strong>（消失梯度）, or sometimes the <strong>exploding gradient</strong>（爆发梯度）. When that happens, training takes too long and the accuracy really suffers. Let’s take a closer look.</p>
<p>When you’re training a neural net, you’re constantly calculating a cost value. The cost is typically the difference between the net’s predicted output and the actual output from a set of labelled training data. The cost is then lowered by making slight adjustments to the weights and biases over and over throughout the training process, until the lowest possible value is obtained. Here is that forward prop again; and here are the example weights and biases.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-9f1aa39cff6f8bd6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>The training process utilizes something called a <strong>gradient</strong>（梯度）, which measures the rate at which the cost will change with respect to a change in a weight or a bias.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-32682b72029d39d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p><strong>Deep architectures</strong> are your best and sometimes your only choice for complex machine learning problems such as facial recognition. But up until 2006, there was no way to accurately train deep nets due to a fundamental problem with the training process: the <strong>vanishing gradient</strong>.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-51768f97898e6cec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Let’s think of a gradient like a slope, and the training process like a rock rolling down that slope. A rock will roll quickly down a steep slope but will barely move at all on a flat surface. The same is true with the gradient of a deep net. When the gradient is large, the net will train quickly. When the gradient is small, the net will train slowly.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-10dc44a68b2f3eeb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Here’s that deep net again.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-f662f490a0d10129.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>And here is how the gradient could potentially vanish or decay back through the net. As you can see, the gradients are much small in the earlier layers. As a result, the early layers of the network are the slowest to train. But this is a fundamental problem! The early layers are responsible for detecting the simple patterns and the building blocks -when it came to facial recognition, the early layers detected the edges which were combined to form facial features later in the network. </p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-4eb36da118574ae0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>And if the early layers get it wrong, the result built up by the net will be wrong as well. It could mean that instead of a face like this, your net looks for this.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-6779922e5ad7b679.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>The process used for training a neural net is called <strong>back-propagation</strong> or <strong>back-prop</strong>（反向传播）. We saw before that <strong>forward prop</strong> starts with the inputs and works forward; <strong>back-prop</strong> does the reverse, calculating the gradient from right to left.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-6f8e9ff65bf631f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>For example, here are 5 gradients, 4 weights and 1 bias. </p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-23bc396e09d8139f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>It starts with the left and works back through the layers, like so. Each time it calculates a gradient, it uses all the previous gradients up to that point.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-44f697c99f9eabd0.gif?imageMogr2/auto-orient/strip" alt=""></p>
<p>So, let’s start with that node. That edge uses the gradient at that node. And the next. So far things are simple.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-a2d602cf560907fd.gif?imageMogr2/auto-orient/strip" alt=""></p>
<p>As you keep going back, things get a bit more complex - that one for example uses a lot of gradients, even though this is a relatively simple net. If your net gets larger and deeper, like this one, it gets even worse.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-de5c3d9af77a018b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>But why is that? Well, a gradient at any point is the product of the previous gradients up to that point. And the product of two numbers between 0 and 1 gives you a smaller number.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-a7a4f7af3c2a6410.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p> Say this rectangle is a one. </p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-265816c3fc9d282e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Also, say there are two gradients - a fourth - like that</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-e99e26fe47452fe4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<ul>
<li>and a third.</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-b09b63e8b060056c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>If you multiply them, you get a fourth of a third which is a twelfth.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-e27dc07fcdbeaa49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>A fourth of a twelfth is a forty-eighth.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-15d7e3479beea24a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>You can see that numbers keep getting smaller the more you multiply. Have you ever had this issue while training a neural network with <strong>backpropagation</strong>? If so, please comment and let me know your thoughts.</p>
<p>As a result of all this, <strong>backprop</strong> ends up taking a lot of time to train the net, and the accuracy is often very low.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-64c3c64229a301a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Up until 2006, deep nets were still underperforming shallow nets and other machine learning algorithms. But everything changed after three breakthrough papers published by Hinton, Lecun, and Bengio in 2006 and 2007.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-4391e6c11ad44c66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>In the next video, we’ll begin taking a closer look at these breakthroughs, starting with the <strong>Restricted Boltzmann Machine</strong>（受限波兹曼机）.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-f7ab81d3c318996b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><p><a href="https://www.youtube.com/watch?v=SKMpmAOUa2Q&amp;index=5&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu" target="_blank" rel="external">视频链接</a><br><a href="http://www.jianshu.com/p/4f7e4f27dad9" target="_blank" rel="external">本系列文章目录</a></p>
<h1 id="拓展阅读"><a href="#拓展阅读" class="headerlink" title="拓展阅读"></a>拓展阅读</h1><ul>
<li><a href="http://blog.csdn.net/qq_29133371/article/details/51867856" target="_blank" rel="external">关于梯度消失，梯度爆炸的问题</a></li>
<li><a href="https://my.oschina.net/findbill/blog/529001" target="_blank" rel="external">反向传播（Backpropagation）算法的数学原理</a></li>
<li><a href="https://www.zhihu.com/question/27239198" target="_blank" rel="external">如何直观的解释back propagation算法？</a></li>
<li><a href="http://blog.csdn.net/dingchenxixi/article/details/51371641" target="_blank" rel="external">Coursera机器学习-第四周-Neural Network ForwardPropagation</a></li>
</ul>
]]></content:encoded>
      
      <comments>http://inerdstack.com/2017/04/02/10787217/#disqus_thread</comments>
    </item>
    
    <item>
      <title>【门外汉深度学习-4】深度网络的选择</title>
      <link>http://inerdstack.com/2017/04/02/10765634/</link>
      <guid>http://inerdstack.com/2017/04/02/10765634/</guid>
      <pubDate>Sun, 02 Apr 2017 04:03:54 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本文源自YouTube网站《&lt;a href=&quot;https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;amp
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文源自YouTube网站《<a href="https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;index=1&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu" target="_blank" rel="external">Deep Learning SIMPLIFIED</a>》系列视频，文章内容为视频的字幕，供深度学习初学者参考学习。</p>
<h1 id="主体内容"><a href="#主体内容" class="headerlink" title="主体内容"></a>主体内容</h1><p><img src="http://upload-images.jianshu.io/upload_images/291600-619b127bca41e211.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Now if you’re like me, starting a Deep Learning project sounds really exciting. But when it comes to picking the right kind of net to use, well, things can get a little confusing. You first need to decide if you’re trying to build a classifier or if you’re trying to find patterns in your data. Beyond that, I’ll try to help by giving you some general guidelines.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-76939354a6feb347.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Before we get started, I want to give you a bit of a heads up. I’m going to be using some terminology that may sound a little scary right now, but don’t worry. I’ll cover all these terms in detail in the upcoming videos. If you’re interested in unsupervised learning - that is, you want to extract patterns from a set of unlabelled data - then your best bet is to use either a <strong>Restricted Boltzmann Machine</strong>（受限波兹曼机）, or an <strong>autoencoder</strong>（自编码器）. What type of projects would you need to use a Deep Net for? Please comment and let me know your thoughts.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-7884bb2e63a021b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>If you have labeled data for supervised learning and you want to build a classifier, you have several different options depending on your application.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-823c5c38d048857c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>For text processing tasks like sentiment analysis, parsing, and named entity recognition - use a <strong>Recurrent Net</strong> or a <strong>Recursive Neural Tensor Network</strong>（递归神经张量网络）,  which we’ll refer to as an <strong><em>RNTN</em></strong>. For any language model that operates on the character level, use a <strong>Recurrent Net</strong>.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-b1a4ef267c3ec0cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>For image recognition, use a <strong>Deep Belief Network</strong>（深度信念网络） or a <strong>Convolutional Net</strong>（卷积网络）.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-497cefdaca33e995.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>For object recognition, use a <strong>Convolutional Net</strong> or an <strong><em>RNTN</em></strong>.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-fe7fb3ba4da4746e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Finally, for speech recognition, use a <strong>Recurrent Net</strong>.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-7864a9362e15672b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Ingeneral, <strong>Deep Belief Networks</strong> and <strong>Multilayer Perceptrons</strong>（多层感知机） with rectified linear units - also known as <strong><em>RELU</em></strong> - are both good choices for classification. For time series analysis, it’s best to use a <strong>Recurrent Net</strong>. </p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-edc84d593ae3c095.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Deep Nets are the current state of the art in pattern recognition, but it’s worth noting that neural nets have been around for decades.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-da7f39818a2bacf7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p> So you might be wondering: why did it take almost 50 years for Deep Nets to come on to the scene? Well, as it turns out, Deep Nets are very hard to train, which we will see in the next video.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-f7ab81d3c318996b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><p><a href="https://www.youtube.com/watch?v=JjZDoojyzXQ&amp;index=4&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu" target="_blank" rel="external">视频链接</a><br><a href="http://www.jianshu.com/p/4f7e4f27dad9" target="_blank" rel="external">本系列文章目录</a></p>
<h1 id="拓展阅读"><a href="#拓展阅读" class="headerlink" title="拓展阅读"></a>拓展阅读</h1><ul>
<li><a href="http://www.cnblogs.com/kemaswill/p/3266026.html" target="_blank" rel="external">Deep Belief Network简介</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_69e2aa270102ygvh.html" target="_blank" rel="external">CNN&amp;RELU&amp;POOL 感觉没有比这篇文章讲的更详细了</a></li>
</ul>
]]></content:encoded>
      
      <comments>http://inerdstack.com/2017/04/02/10765634/#disqus_thread</comments>
    </item>
    
    <item>
      <title>【门外汉深度学习-3】为什么要深入学习</title>
      <link>http://inerdstack.com/2017/04/02/10682963/</link>
      <guid>http://inerdstack.com/2017/04/02/10682963/</guid>
      <pubDate>Sun, 02 Apr 2017 04:01:56 GMT</pubDate>
      <description>
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本文源自YouTube网站《&lt;a href=&quot;https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;amp
      
      </description>
      
      <content:encoded><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文源自YouTube网站《<a href="https://www.youtube.com/watch?v=b99UVkWzYTQ&amp;index=1&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu" target="_blank" rel="external">Deep Learning SIMPLIFIED</a>》系列视频，文章内容为视频的字幕，供深度学习初学者参考学习。</p>
<h1 id="主体内容"><a href="#主体内容" class="headerlink" title="主体内容"></a>主体内容</h1><p>If you want your computer to recognize very complex patterns - then trust me on this - you really need to start using neural networks. </p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-5096f18440ed94c1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>When the patterns get really complex, neural nets start  to outperform all of their competition. Plus, GPUs can train them faster than ever before! Let’s take a look. Neural  nets truly have the potential to revolutionize the field of Artificial Intelligence. We all know that computers are very good with repetitive calculations and detailed instructions, but they’re historically been bad at recognizing patterns. Thanks to deep learning, this is all about to change.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-bda7fa1f3217fe70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>If you only need to analyze simple patterns, a basic classification tool like an SVM or Logistic Regression is typically good enough. But when your data has 10s of different inputs or more, neural nets start to win out over the other methods. Still, as the patterns get even more complex, neural networks with a small number of layers can become unusable. The reason is that the number of nodes required in each layer grows exponentially with the number of possible patterns in the data. Eventually training becomes way too expensive and the accuracy starts to suffer. So for an intricate pattern - like an image of a human face, for example - basic classification engines and shallow neural nets simply aren’t good enough - the only practical choice is a deep net. Have you ever run into a wall when trying to work with highly complex data? Please comment and let me know your thoughts.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-33521a1b7a18caef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>But what enables a deep net to recognize these complex patterns? The key is that deep nets are able to break the complex patterns down into a series of simpler patterns. </p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-e44582f5ce31488b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>For example, let’s say that a net had to decide whether or not an image contained a human face. A deep net would first use edges to detect different parts of the face - the lips, nose, eyes, ears, and so on - and would then combine the results together to form the whole face. This important feature - using simpler patterns as building blocks to detect complex patterns - is what gives you deep nets their strength. The accuracy of these nets has become very impressive - in fact, a deep net from google recently beat a human at a pattern recognition challenge. </p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-6ec9a3e1526696ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>It’s not surprising that deep nets were inspired by the structure of our own human brains. Even in the early days of neural networks, researchers wanted to link a large number of perceptrons together in a layered web - an idea which helped improve their accuracy. It is believed that our brains have a very deep architecture and that we decipher patterns just like a deep net - we detect complex patterns by first detecting, and combining the simple one.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-003f76776875f6ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>There is one downside to all of this - deep nets take much longer to train. The good news is that recent advances in computing have really reduced the amount of time it takes to properly train a net. High performance GPUs can finish training a complex net in under a week, when fast CPUs may have taken weeks or even months.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-758d196573c9e134.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<p>Before we talk more about the various Deep Learning models, we’re going to briefly discuss which types of deep nets are suitable for different machine learning tasks. That’s coming in the next video.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/291600-f7ab81d3c318996b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/350" alt=""></p>
<h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><p><a href="https://www.youtube.com/watch?v=CEv_0r5huTY&amp;index=3&amp;list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu" target="_blank" rel="external">视频链接</a><br><a href="http://www.jianshu.com/p/4f7e4f27dad9" target="_blank" rel="external">本系列文章目录</a></p>
]]></content:encoded>
      
      <comments>http://inerdstack.com/2017/04/02/10682963/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
